Here's a step-by-step guide to train a **ResNet50** model on a custom image dataset and extract **2048-dimensional embeddings** using PyTorch:

---

### **Step 1: Install Required Libraries**
```bash
pip install torch torchvision pillow numpy pandas tqdm
```

---

### **Step 2: Prepare Custom Dataset**
Assume your dataset is structured as:
```
custom_dataset/
â”œâ”€â”€ class_1/
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”œâ”€â”€ img2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ class_2/
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

#### **Define Dataset Class**
```python
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os

class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = os.listdir(root_dir)
        self.images = []
        
        for class_name in self.classes:
            class_path = os.path.join(root_dir, class_name)
            for img_name in os.listdir(class_path):
                self.images.append((os.path.join(class_path, img_name), self.classes.index(class_name)))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path, label = self.images[idx]
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
            
        return image, label

# Define transformations
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load dataset
dataset = CustomDataset(root_dir="custom_dataset/", transform=transform)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

---

### **Step 3: Load Pre-trained ResNet50 & Modify for Embeddings**
```python
import torchvision.models as models
import torch.nn as nn

# Load pre-trained ResNet50
model = models.resnet50(pretrained=True)

# Remove the final classification layer (fc)
model = torch.nn.Sequential(*(list(model.children())[:-1]))  # Output: 2048-d embedding

# Set model to evaluation mode
model.eval()

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
```

---

### **Step 4: Extract Embeddings**
```python
import numpy as np

def extract_embeddings(dataloader, model):
    embeddings = []
    labels = []
    
    with torch.no_grad():
        for images, batch_labels in dataloader:
            images = images.to(device)
            batch_embeddings = model(images)
            batch_embeddings = batch_embeddings.squeeze()  # Remove extra dimensions
            
            embeddings.append(batch_embeddings.cpu().numpy())
            labels.append(batch_labels.numpy())
    
    return np.concatenate(embeddings), np.concatenate(labels)

embeddings, labels = extract_embeddings(dataloader, model)
print("Embeddings shape:", embeddings.shape)  # (N, 2048)
```

---

### **Step 5: Save Embeddings**
```python
import pandas as pd

# Save as CSV
df = pd.DataFrame(embeddings)
df['label'] = labels
df.to_csv("image_embeddings.csv", index=False)

# Or save as numpy file
np.save("embeddings.npy", embeddings)
np.save("labels.npy", labels)
```

---

### **Step 6: Use Embeddings for Similarity Search**
You can now use these embeddings in **Qdrant, FAISS, or scikit-learn** for similarity search:
```python
from sklearn.neighbors import NearestNeighbors

# Fit KNN
nn = NearestNeighbors(n_neighbors=5)
nn.fit(embeddings)

# Query similar images
query_embedding = embeddings[0].reshape(1, -1)  # Example query
distances, indices = nn.kneighbors(query_embedding)
print("Similar images:", indices)
```

---

### **Key Notes**
1. **Training (Fine-tuning)**:  
   If you want to fine-tune ResNet50 on your dataset, replace the last layer and train:
   ```python
   model.fc = nn.Linear(2048, num_classes)  # Your number of classes
   criterion = nn.CrossEntropyLoss()
   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
   ```
   Then train using a standard PyTorch training loop.

2. **Batch Processing**:  
   For large datasets, process in batches to avoid memory issues.

3. **GPU Acceleration**:  
   Use `model.to(device)` for faster inference.

4. **Dimensionality Reduction**:  
   Use PCA or UMAP to reduce 2048-d embeddings to lower dimensions if needed.

This pipeline gives you **2048-d ResNet50 embeddings** ready for search/retrieval tasks. ðŸš€