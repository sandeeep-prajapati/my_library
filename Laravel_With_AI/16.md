Here's a comprehensive solution to periodically update your deep learning model with new data in a production Laravel + Python environment:

---

### **1. Python Training Service (`ml_service/train.py`)**
```python
import torch
from torch.utils.data import Dataset, ConcatDataset, DataLoader
from torchvision import transforms, models
import numpy as np
from datetime import datetime
from fastapi import BackgroundTasks
import os
from PIL import Image

class ProductDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.image_paths = [...]  # Load from your data source
        self.labels = [...]       # Corresponding labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert('RGB')
        if self.transform:
            img = self.transform(img)
        return img, self.labels[idx]

def train_model(background_tasks: BackgroundTasks):
    # Initialize model
    model = models.resnet50(pretrained=True)
    model.fc = torch.nn.Linear(2048, 10)  # Adjust for your classes
    
    # 1. Load existing and new datasets
    transform = transforms.Compose([...])  # Your transforms
    
    old_data = ProductDataset('data/processed/v1', transform)
    new_data = ProductDataset('data/processed/v2', transform)
    
    # Combine datasets
    combined_data = ConcatDataset([old_data, new_data])
    loader = DataLoader(combined_data, batch_size=64, shuffle=True)
    
    # 2. Training setup
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    model.train()
    
    # 3. Training loop
    for epoch in range(10):
        for images, labels in loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
    
    # 4. Save updated model
    version = datetime.now().strftime("%Y%m%d_%H%M")
    model_path = f"models/resnet50_{version}.pt"
    torch.save(model.state_dict(), model_path)
    
    # Schedule model deployment
    background_tasks.add_task(deploy_model, model_path)

def deploy_model(model_path):
    # Your deployment logic here
    # Example: Move to production directory, update API endpoints, etc.
    os.rename(model_path, "models/current/production.pt")
```

---

### **2. FastAPI Endpoint for Triggering Updates**
```python
# ml_service/main.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI()

class TrainingRequest(BaseModel):
    force: bool = False

@app.post("/update-model")
async def update_model(
    request: TrainingRequest,
    background_tasks: BackgroundTasks
):
    if should_retrain(request.force):
        background_tasks.add_task(train_model, background_tasks)
        return {"status": "training_started"}
    return {"status": "no_update_needed"}

def should_retrain(force: bool) -> bool:
    # Implement your retraining logic:
    # - Check for new data
    # - Model performance drift
    # - Scheduled time
    return force or check_new_data_exists()
```

---

### **3. Laravel Scheduled Command**
```bash
php artisan make:command TriggerModelUpdate
```

```php
<?php

namespace App\Console\Commands;

use Illuminate\Console\Command;
use Illuminate\Support\Facades\Http;

class TriggerModelUpdate extends Command
{
    protected $signature = 'model:update {--force}';
    protected $description = 'Trigger deep learning model retraining';

    public function handle()
    {
        $response = Http::timeout(300)
            ->post(config('services.ml.training_url'), [
                'force' => $this->option('force')
            ]);

        if ($response->successful()) {
            $this->info($response->json()['status']);
        } else {
            $this->error('Training trigger failed');
        }
    }
}
```

---

### **4. Configure Scheduled Updates**
```php
// app/Console/Kernel.php
protected function schedule(Schedule $schedule)
{
    // Weekly retraining
    $schedule->command('model:update')
        ->weekly()
        ->appendOutputTo(storage_path('logs/model_training.log'));
    
    // Force retrain when needed
    // $schedule->command('model:update --force')->daily();
}
```

---

### **5. Model Version Tracking**
Create a database table to track model versions:

```bash
php artisan make:migration create_model_versions_table
```

```php
Schema::create('model_versions', function (Blueprint $table) {
    $table->id();
    $table->string('version');
    $table->string('path');
    $table->json('metrics'); // accuracy, loss, etc.
    $table->timestamp('trained_at');
    $table->boolean('is_production')->default(false);
});
```

---

### **6. Deployment Workflow**

1. **Training Phase**:
   ```python
   # After training completes:
   def deploy_model(model_path):
       # 1. Validate model performance
       test_accuracy = evaluate_model(model_path)
       
       # 2. Record in database
       record_model_version(
           version=os.path.basename(model_path),
           accuracy=test_accuracy
       )
       
       # 3. Deploy to production
       if test_accuracy > threshold:
           update_production_model(model_path)
   ```

2. **Blue-Green Deployment**:
   ```php
   // Laravel config
   'model_version' => env('MODEL_VERSION', '20230101_0000')
   ```

---

### **7. Monitoring & Rollback**

#### **Model Performance Dashboard**
```php
// ModelVersionController.php
public function index()
{
    return view('models.dashboard', [
        'versions' => ModelVersion::orderBy('trained_at', 'desc')->get()
    ]);
}
```

#### **Rollback Command**
```bash
php artisan model:rollback 20230101_0000
```

---

### **Key Components**

1. **Data Pipeline**:
   - Automatic dataset combination with `ConcatDataset`
   - Versioned data storage

2. **Training Process**:
   - Background training to avoid API downtime
   - Model checkpointing

3. **Deployment Safety**:
   - Performance validation before deployment
   - Version tracking for rollback

4. **Scheduling**:
   - Regular automatic updates
   - Manual override with `--force`

---

### **Example Workflow**

1. **New data arrives** in `data/processed/v2/`
2. **Scheduled task runs**:
   ```bash
   php artisan model:update
   ```
3. **Python service**:
   - Combines v1 + v2 data
   - Trains for 10 epochs
   - Saves as `models/resnet50_20230101_1200.pt`
4. **Deployment**:
   - Validates accuracy > 95%
   - Updates production model
   - Logs version in database

---

### **Performance Optimization**

1. **Incremental Training**:
   ```python
   # Load previous model weights
   model.load_state_dict(torch.load('models/current.pt'))
   ```

2. **Distributed Training**:
   ```python
   torch.nn.DataParallel(model)  # Multi-GPU
   ```

3. **Dataset Caching**:
   ```python
   dataset = ProductDataset(...).cache()  # Implement caching
   ```

This implementation provides a **production-ready** model update pipeline with:
- Zero-downtime updates
- Version control
- Performance monitoring
- Rollback capability