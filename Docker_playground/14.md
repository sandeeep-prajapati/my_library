## ğŸ“ Project structure

```text
ml-inference/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ main.py
```

---

## ğŸ§© docker-compose.yml

version: "3.9"

services:
api:
build: ./api
expose:
- "8000"
environment:
- REDIS_HOST=redis
- REDIS_PORT=6379
depends_on:
- redis
deploy:
replicas: 3
networks:
- ml_net

redis:
image: redis:7-alpine
container_name: redis_cache
ports:
- "6379:6379"
networks:
- ml_net

nginx:
image: nginx:alpine
container_name: nginx_lb
ports:
- "80:80"
volumes:
- ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
depends_on:
- api
networks:
- ml_net

networks:
ml_net:
driver: bridge

---

## ğŸ³ api/Dockerfile (PyTorch + FastAPI)

```dockerfile
FROM pytorch/pytorch:2.1.0-cpu

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸ“¦ api/requirements.txt

```text
fastapi
uvicorn
torch
redis
pydantic
```

---

## ğŸ§  api/main.py (Inference + Redis cache)

```python
from fastapi import FastAPI
import redis
import torch
import hashlib

app = FastAPI()
r = redis.Redis(host="redis", port=6379, decode_responses=True)

@app.get("/predict")
def predict(text: str):
    key = hashlib.md5(text.encode()).hexdigest()

    cached = r.get(key)
    if cached:
        return {"source": "cache", "result": cached}

    # Dummy inference
    result = f"prediction_for_{text}"

    r.setex(key, 3600, result)
    return {"source": "model", "result": result}
```

---

## ğŸŒ nginx/nginx.conf (Load Balancer)

```nginx
events {}

http {
  upstream inference_api {
    least_conn;
    server api:8000;
  }

  server {
    listen 80;

    location / {
      proxy_pass http://inference_api;
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
    }
  }
}
```

---

## ğŸš€ Run the stack

```bash
docker compose up --build
```

Test:

```bash
curl "http://localhost/predict?text=hello"
```

---

## ğŸ§  Why this architecture works in real systems

### âœ… FastAPI

* Async, low-latency inference
* Auto Swagger docs

### âœ… Redis cache

* Avoids repeated model inference
* Massive latency reduction

### âœ… Nginx load balancing

* Scales horizontally
* Healthier traffic distribution

### âœ… Service discovery

* Uses Docker DNS (`api`, `redis`)
* No hardcoded IPs

