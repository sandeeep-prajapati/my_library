Here is a **clean, production-style mini-project** in Go that uses **goroutines + worker pool + buffered channels** to scrape **20 URLs concurrently**.

This example:

* Uses **goroutines** for concurrency
* Uses a **worker pool** (fast + controlled parallelism)
* Uses **channels** for job distribution + results
* Uses `net/http` with timeouts
* Handles errors gracefully

---

# ðŸš€ **Mini Project: Concurrent Web Scraper (20 URLs)**

**Files:**

* `main.go`

---

# âœ… **main.go**

```go
package main

import (
	"fmt"
	"io/ioutil"
	"net/http"
	"time"
)

// Worker function
func worker(id int, jobs <-chan string, results chan<- string) {
	client := http.Client{
		Timeout: 5 * time.Second,
	}

	for url := range jobs {
		resp, err := client.Get(url)
		if err != nil {
			results <- fmt.Sprintf("[Worker %d] Error fetching %s: %v", id, url, err)
			continue
		}

		body, _ := ioutil.ReadAll(resp.Body)
		resp.Body.Close()

		results <- fmt.Sprintf("[Worker %d] %s -> %d bytes", id, url, len(body))
	}
}

func main() {
	// 20 test URLs
	urls := []string{
		"https://example.com", "https://httpbin.org/get", "https://golang.org",
		"https://api.github.com", "https://jsonplaceholder.typicode.com/todos/1",
		"https://reqres.in/api/users", "https://dummyjson.com/products",
		"https://httpbin.org/uuid", "https://httpbin.org/ip", "https://httpbin.org/headers",
		"https://example.org", "https://httpstat.us/200", "https://httpstat.us/503",
		"https://httpstat.us/404", "https://httpstat.us/500", "https://icanhazip.com",
		"https://worldtimeapi.org/api/timezone/Etc/UTC",
		"https://jsonplaceholder.typicode.com/posts",
		"https://jsonplaceholder.typicode.com/users",
		"https://jsonplaceholder.typicode.com/comments",
	}

	jobs := make(chan string, len(urls))
	results := make(chan string, len(urls))

	workerCount := 5 // number of goroutines

	// Start workers
	for w := 1; w <= workerCount; w++ {
		go worker(w, jobs, results)
	}

	// Send URLs to jobs channel
	for _, url := range urls {
		jobs <- url
	}
	close(jobs)

	// Collect results
	for i := 0; i < len(urls); i++ {
		fmt.Println(<-results)
	}
}
```

---

# ðŸ§  **How It Works**

### **1ï¸âƒ£ Worker Pool**

You start `5` workers:

```go
for w := 1; w <= workerCount; w++ {
    go worker(w, jobs, results)
}
```

Each worker:

* Waits for a URL from `jobs`
* Fetches the website
* Sends the result to `results`

---

### **2ï¸âƒ£ Job Queue**

Buffered channel holds all 20 URLs:

```go
jobs := make(chan string, len(urls))
```

---

### **3ï¸âƒ£ Results Queue**

Collect scraped page size or errors:

```go
results := make(chan string, len(urls))
```

---

### **4ï¸âƒ£ Safe Concurrency**

Workers pull jobs **asynchronously**, so fast workers take more tasks.

This is exactly how production scrapers, crawlers, and pipelines work.

---

# ðŸ“Œ Example Output (varies)

```
[Worker 2] https://example.com -> 1256 bytes
[Worker 1] https://httpbin.org/get -> 346 bytes
[Worker 4] https://golang.org -> 8900 bytes
[Worker 3] https://api.github.com -> 2345 bytes
[Worker 5] https://dummyjson.com/products -> 12000 bytes
...
```

---
